Large Language Models (LLMs) have been attracting much attention in recent years due to their remarkable performance in various natural language processing (NLP) and reasoning tasks [Zhao '23, GPTo1 strawberry]. To achieve ever-greater performance, LLMs have been growing in size, typically boasting a few billion [P. Zhang '24] to hundreds of billions of parameters [Dubey '24]. A core value of LLMs lies in their ability to digest and reason over vast amounts of data in the form of large text documents and multi-modal data such as images, audio or videos [?]. Therefore, more recently, model context length has been increasing rapidly to enable these tasks, and now range from a few thousands to millions of tokens [Hooper '24, Gemini Pro]. However, extreme context length put a significant strain on the memory and compute resources during inference. For instance, studies have shown [?] that for large input lengths, KV cache size dominates the total GPU memory consumption, especially when using model compression techniques [Yuan '24]. [example for cache size in GB for large model, see Hooper '24]. Therefore, there has been great interest in the research community to explore techniques for optimizing the bottlenecks arising from the attention operator and KV cache in long context scenarios.

Sparse attention, Key-Value cache offloading and compression, have proven (?) to be effective at addressing these [?]. A recent paper, PQCache [H. Zhang '24], combines these techniques by offloading the KV cache to the CPU and formulating the retrieval of relevant tokens as a Maximum Inner Product Search (MIPS) problem, making use of Product Quantization to compress key tokens and maintaining model quality.

While reducing overheads by scheduling computations to partially overlap, offloading these operations to the CPU still leads to a significant system-level latency increase, especially in the decoding phase where repetitive retrieval of top-k tokens are being performed [is that the main bottleneck?][?]. FPGAs have been shown to excel at efficient and low-latency retrieval tasks [Chameleon, Understanding the potential ...] like .... To address the ... of PQCache, we aim to explore the potential of enabling both fast and accurate long-context LLM generation by offloading KV cache construction, management and retrieval to an FPGA-based accelerator.

[Other limitations of PQCache or related papers, that should be addressed? What are the limitations of the current state-of-the-art?]


Objectives:
